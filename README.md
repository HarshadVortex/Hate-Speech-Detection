# Hate-Speech-Detection
The widespread occurrence of hate speech and cyberbullying in online communication has become a pressing concern, posing significant threats to individuals and society at large. These harmful behaviours can inflict severe emotional damage, reputational harm, and even lead to acts of violence. this project delves into the creation of a hate speech classification model using Support Vector Machine (SVM) and advanced natural language processing techniques. The code focuses on cleaning and preprocessing tweets to extract meaningful features, utilizing CountVectorizer to transform text data into numerical representations, and employing SVM to identify patterns that distinguish between different types of tweets. The model is evaluated on a labeled dataset and demonstrates promising results in recognizing hate speech and offensive language.


This hate speech detection minor project leverages and identify and categorize hate speech in textual data. The dataset used for training and evaluation contains labeled tweets categorized into three classes: Hate Speech, Offensive Speech, and No Hate and Offensive Speech. The project employs natural language processing techniques to preprocess the textual data, including lowercasing, removal of URLs, punctuation, and stop words, as well as stemming.


![image](https://github.com/HarshadVortex/Hate-Speech-Detection/assets/164507622/3cc8a127-dcd4-4e7c-85eb-199e064b8a39)


This project aims to develop a model using Support Vector Machine (SVM) and natural language processing techniques. The code focuses on cleaning and preprocessing tweets to extract relevant features, employing CountVectorizer to convert text data into numerical representations, and utilizing SVM to learn patterns that distinguish between different types of tweets. The model is evaluated on a labeled dataset and demonstrates promising results in identifying hate speech and offensive language.
The project highlights the potential of machine learning in addressing the challenges posed by hate speech and cyberbullying. By effectively classifying harmful content, such models can aid in moderation efforts, protect individuals from exposure to harmful language
